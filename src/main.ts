import { App, Plugin, PluginSettingTab, TFile, Notice } from 'obsidian';
import { ATTNSettings, AudioSpeedOption, VerboseTranscriptionResult } from './types';
import { ATTNSettingTab } from './settings';
import { ApiService } from './apiService';
import { NoteCreator } from './noteCreator';
import { TemplateProcessor } from './templateProcessor';
import { ConfigLoader } from './configLoader';
import { AudioProcessor } from './audioProcessor';
import { TemplateLoader } from './templateLoader';

const DEFAULT_SETTINGS: ATTNSettings = {
  openaiApiKey: '', // Legacy field for backward compatibility
  saveFolderPath: '/',
  noteFilenameTemplate: '{{date:YYYY-MM-DD}}-{{filename}}-íšŒì˜ë¡',
  noteContentTemplate: '# íšŒì˜ë¡\n\n**ì›ë³¸ íŒŒì¼:** {{filename}}\n**ìƒì„± ë‚ ì§œ:** {{date:YYYY-MM-DD}}\n\n## ìš”ì•½\n\n{{summary}}',
  noteContentTemplateFile: '',
  useTemplateFile: false,
  systemPrompt: 'Please provide a clear and concise summary of the audio transcript. Focus on key points, decisions made, and action items. Please prefer Korean for the summary.',
  audioSpeedMultiplier: 1,
  ffmpegPath: '',
  stt: {
    provider: 'openai',
    model: 'whisper-1',
    language: 'ko',
    whisperBackend: 'faster-whisper-cpp',
    whisperModelPathOrName: 'base'
  },
  summary: {
    provider: 'openai',
    model: 'gpt-4'
  },
  processing: {
    enableChunking: true,
    maxUploadSizeMB: 24.5,
    maxChunkDurationSec: 85,
    targetSampleRateHz: 16000,
    targetChannels: 1,
    silenceThresholdDb: -35,
    minSilenceMs: 400,
    hardSplitWindowSec: 30,
    preserveIntermediates: false,
    diarization: {
      enabled: false,
      provider: 'pyannote',
      minSpeakers: 1,
      maxSpeakers: 10,
      mergeThreshold: 1.0
    }
  },
  logging: {
    enabled: true,
    level: 'error',
    maxLogFileBytes: 5 * 1024 * 1024,
    maxLogFiles: 5
  }
};

export default class ATTNPlugin extends Plugin {
  settings: ATTNSettings;
  private configLoader: ConfigLoader;

  async onload() {
    this.configLoader = ConfigLoader.getInstance();
    await this.loadSettings();

    this.addSettingTab(new ATTNSettingTab(this.app, this));

    this.registerEvent(
      this.app.workspace.on('file-menu', (menu, file) => {
        if (file instanceof TFile && file.extension === 'm4a') {
          menu.addItem((item) => {
            item
              .setTitle('ATTN: ìš”ì•½ ë…¸íŠ¸ ìƒì„±í•˜ê¸°')
              .setIcon('document')
              .onClick(async () => {
                await this.processAudioFile(file);
              });
          });
        }
      })
    );
  }

  onunload() {

  }

  async loadSettings() {
    const loadedData = await this.loadData();
    this.settings = Object.assign({}, DEFAULT_SETTINGS, loadedData);
    
    // Migration logic for backward compatibility
    await this.migrateSettings(loadedData);
  }

  private async migrateSettings(loadedData: any) {
    let needsSave = false;

    // If old openaiApiKey exists but new structure doesn't, migrate it
    if (loadedData?.openaiApiKey && 
        (!loadedData.stt || !loadedData.summary)) {
      
      // Migrate STT settings
      if (!this.settings.stt.apiKey && loadedData.openaiApiKey) {
        this.settings.stt.apiKey = loadedData.openaiApiKey;
        needsSave = true;
      }
      
      // Migrate Summary settings  
      if (!this.settings.summary.apiKey && loadedData.openaiApiKey) {
        this.settings.summary.apiKey = loadedData.openaiApiKey;
        needsSave = true;
      }
    }

    // Add missing processing settings for existing users
    if (!loadedData?.processing) {
      this.settings.processing = DEFAULT_SETTINGS.processing;
      needsSave = true;
    }

    // Add missing logging settings for existing users
    if (!loadedData?.logging) {
      this.settings.logging = DEFAULT_SETTINGS.logging;
      needsSave = true;
    }

    if (needsSave) {
      await this.saveSettings();
    }
  }

  private validateApiKeys(): boolean {
    const configApiKey = this.configLoader.getOpenAIApiKey();
    
    // Check STT provider requirements
    if (this.settings.stt.provider === 'openai' || this.settings.stt.provider === 'gemini') {
      const sttApiKey = this.settings.stt.apiKey || this.settings.openaiApiKey || configApiKey;
      if (!sttApiKey || sttApiKey.trim() === '') {
        new Notice(`${this.settings.stt.provider.toUpperCase()} STT API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”ŒëŸ¬ê·¸ì¸ ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”.`);
        console.error(`${this.settings.stt.provider.toUpperCase()} STT API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`);
        return false;
      }
    }

    // Check local whisper requirements
    if (this.settings.stt.provider === 'local-whisper') {
      if (!this.settings.stt.ollamaEndpoint && !this.settings.stt.whisperBinaryPath) {
        new Notice('Local Whisper ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” Ollama ì—”ë“œí¬ì¸íŠ¸ ë˜ëŠ” Whisper ë°”ì´ë„ˆë¦¬ ê²½ë¡œë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.');
        console.error('Local Whisper ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
        return false;
      }
    }

    // Check Summary provider requirements
    if (this.settings.summary.provider === 'openai' || this.settings.summary.provider === 'gemini') {
      const summaryApiKey = this.settings.summary.apiKey || this.settings.openaiApiKey || configApiKey;
      if (!summaryApiKey || summaryApiKey.trim() === '') {
        new Notice(`${this.settings.summary.provider.toUpperCase()} Summary API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”ŒëŸ¬ê·¸ì¸ ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”.`);
        console.error(`${this.settings.summary.provider.toUpperCase()} Summary API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`);
        return false;
      }
    }

    // Check local LLM requirements
    if (this.settings.summary.provider === 'local-llm') {
      if (!this.settings.summary.ollamaEndpoint) {
        new Notice('Local LLM ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” Ollama ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.');
        console.error('Local LLM ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
        return false;
      }
    }

    return true;
  }

  async saveSettings() {
    await this.saveData(this.settings);
  }

  async processAudioFile(file: TFile) {
    try {
      // Check if required API keys are configured based on provider selection
      const hasRequiredKeys = this.validateApiKeys();
      
      if (!hasRequiredKeys) {
        return; // validateApiKeys will show appropriate error notice
      }

      if (this.configLoader.isDebugMode()) {
        console.log('ðŸ”§ ATTN Debug: Processing audio file:', file.name);
        console.log('ðŸ”§ ATTN Debug: Config file path:', this.configLoader.getConfigPath());
      }

      // Show progress notice
      const processingNotice = new Notice('ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ìžˆìŠµë‹ˆë‹¤...', 0);

      // Step 1: Read audio file
      const audioData = await this.app.vault.readBinary(file);
      let audioFile = new File([audioData], file.name, { type: 'audio/m4a' });

      // Step 2: Process audio speed if necessary
      if (this.settings.audioSpeedMultiplier > 1) {
        try {
          processingNotice.setMessage(`ì˜¤ë””ì˜¤ ì†ë„ ì²˜ë¦¬ ì¤‘... (${this.settings.audioSpeedMultiplier}ë°°ì†)`);
          const audioProcessor = new AudioProcessor(this.settings.ffmpegPath);
          
          // Check if ffmpeg is available
          const ffmpegAvailable = await audioProcessor.checkFFmpegAvailability();
          if (!ffmpegAvailable) {
            new Notice('âš ï¸ FFmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ FFmpeg ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì›ë³¸ ì†ë„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.');
            console.warn('FFmpeg not available at configured path, processing at original speed');
          } else {
            audioFile = await audioProcessor.processAudioSpeed(audioFile, this.settings.audioSpeedMultiplier as AudioSpeedOption);
            if (this.configLoader.isDebugMode()) {
              console.log(`ðŸ”§ ATTN Debug: Audio processed at ${this.settings.audioSpeedMultiplier}x speed`);
            }
          }
        } catch (error) {
          console.warn('Audio speed processing failed, using original file:', error);
          new Notice('âš ï¸ ì˜¤ë””ì˜¤ ì†ë„ ì²˜ë¦¬ ì‹¤íŒ¨, ì›ë³¸ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.');
        }
      }

      // Step 3: Process with API service using new provider system
      processingNotice.setMessage('ìŒì„± ì¸ì‹ ë° ìš”ì•½ ìƒì„± ì¤‘...');
      const apiService = new ApiService(this.settings);
      const result = await apiService.processAudioFile(audioFile, this.settings.systemPrompt);

      // Step 4: Prepare template data
      const templateData = {
        filename: file.name,
        transcript: result.transcript,
        summary: result.summary,
        speakers: this.formatSpeakers(result.transcriptionResult),
        speakerTranscript: this.formatSpeakerTranscript(result.transcriptionResult),
      };

      // Step 5: Process templates using TemplateProcessor and TemplateLoader
      const templateProcessor = new TemplateProcessor();
      const templateLoader = new TemplateLoader(this.app.vault);
      
      // Generate filename using template
      const generatedFileName = templateProcessor.process(
        this.settings.noteFilenameTemplate,
        templateData
      );
      
      // Load content template (from file or fallback to inline template)
      processingNotice.setMessage('í…œí”Œë¦¿ ì²˜ë¦¬ ì¤‘...');
      const contentTemplate = await templateLoader.getTemplateContent(
        this.settings.useTemplateFile,
        this.settings.noteContentTemplateFile,
        this.settings.noteContentTemplate
      );
      
      // Generate content using template
      const generatedContent = templateProcessor.process(
        contentTemplate,
        templateData
      );

      // Step 4: Construct full file path
      let fullPath: string;
      if (this.settings.saveFolderPath === '/' || this.settings.saveFolderPath === '') {
        fullPath = `${generatedFileName}.md`;
      } else {
        // Remove leading/trailing slashes and ensure proper format
        const cleanFolderPath = this.settings.saveFolderPath.replace(/^\/+|\/+$/g, '');
        fullPath = `${cleanFolderPath}/${generatedFileName}.md`;
      }

      // Step 5: Create the note
      const noteCreator = new NoteCreator(this.app.vault);
      await noteCreator.createNote(fullPath, generatedContent);

      // Hide progress notice and show success
      processingNotice.hide();
      new Notice(`íšŒì˜ë¡ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: ${fullPath}`);

    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);
      
      let errorMessage = 'ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
      if (error instanceof Error) {
        errorMessage += ` ${error.message}`;
      }
      
      new Notice(errorMessage);
    }
  }

  /**
   * Format speakers list for template
   */
  private formatSpeakers(transcriptionResult: VerboseTranscriptionResult): string {
    if (!transcriptionResult.speakers || transcriptionResult.speakers.length === 0) {
      return '';
    }

    return transcriptionResult.speakers
      .map((speaker: any) => `- ${speaker.label}`)
      .join('\n');
  }

  /**
   * Format transcript with speaker labels for template
   */
  private formatSpeakerTranscript(transcriptionResult: VerboseTranscriptionResult): string {
    if (!transcriptionResult.segments) {
      return transcriptionResult.text || '';
    }

    const groupedSegments = this.groupSegmentsBySpeaker(transcriptionResult.segments);
    
    return groupedSegments
      .map(group => {
        const speakerLabel = group.speaker ? `**${group.speaker.label}:** ` : '';
        return `${speakerLabel}${group.text}`;
      })
      .join('\n\n');
  }

  /**
   * Group consecutive segments by the same speaker
   */
  private groupSegmentsBySpeaker(segments: any[]): Array<{speaker: any, text: string}> {
    const groups: Array<{speaker: any, text: string}> = [];
    let currentGroup: {speaker: any, text: string} | null = null;

    for (const segment of segments) {
      if (!currentGroup || 
          (currentGroup.speaker?.id !== segment.speaker?.id)) {
        // New speaker or no previous group
        if (currentGroup) {
          groups.push(currentGroup);
        }
        currentGroup = {
          speaker: segment.speaker,
          text: segment.text
        };
      } else {
        // Same speaker, append text
        currentGroup.text += ' ' + segment.text;
      }
    }

    if (currentGroup) {
      groups.push(currentGroup);
    }

    return groups;
  }
}